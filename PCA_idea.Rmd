---
title: "NOx_Emulator_Dev"
author: "Elliot Maceda"
date: '2024-03-20'
output: html_document
---


```{r Experimental PCA Idea}

# problems with... 2008 and 2015
# 2008, August
# 2015, May

eigen_data = csv_from_nc_data_v3(daily_nc_directory = "D:/CTM_daily_nc",
                            years_to_take = c("2006", "2007", "2009", "2010", "2011", "2012", "2013","2014", "2016"),
                            # years_to_take = c("2015"),
                            months_to_take = c("01", "02", "03", "04", "05","06", "07", "08", "09", "10", "11", "12"),
                            lat_locations_to_take = c(10.8, 50.5),
                            lon_locations_to_take = c(100, 154.8),
                            # emissions_to_include = c(),
                            filter_negative_satellite_conc = FALSE,
                            additional_predictor_variables = c("all", "BIOGSRCE__ISOP", "BIOGSRCE__ACET", "BIOGSRCE__PRPE", "BIOGSRCE__MONOT", "BIOGSRCE__MBO"), # 
                            area_csv_file_path = "C:/Users/71000/Desktop/NOx_Emulator/area_data/area.csv",
                            avg_population_density_csv_file_path = "C:/Users/71000/Desktop/NOx_Emulator/population_data/density/avg_population_density_data.csv",
                            total_population_count_csv_file_path = "C:/Users/71000/Desktop/NOx_Emulator/population_data/count/total_population_count_data.csv",
                            only_land = TRUE)

interpretable_data = eigen_data$interpretable_data
standardized_data = eigen_data$data

check_num_locations = interpretable_data %>% 
  select(LAT, LON)


print("total number of locations")
print(nrow(unique(check_num_locations)))

Prior_emissions = interpretable_data %>% 
  select(LAT, LON, Prior_Estimates, Day)
Prior_emissions_wide = pivot_wider(Prior_emissions, names_from = c(LAT, LON), values_from = Prior_Estimates)[,-1]

SAT_conc = interpretable_data %>% 
  select(LAT, LON, Satellite_Concentrations, Day)
SAT_conc_wide = pivot_wider(SAT_conc, names_from = c(LAT, LON), values_from = Satellite_Concentrations)[,-1]

#### Predictors # 

CTM_conc = interpretable_data %>% 
  select(LAT, LON, CTM_Concentrations, Day)
CTM_conc_wide = pivot_wider(CTM_conc, names_from = c(LAT, LON), values_from = CTM_Concentrations)[,-1]

RADSWG = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__RADSWG, Day)
RADSWG_wide = pivot_wider(RADSWG, names_from = c(LAT, LON), values_from = DAO_FLDS__RADSWG)[,-1]

PBL = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__PBL, Day)
PBL_wide = pivot_wider(PBL, names_from = c(LAT, LON), values_from = DAO_FLDS__PBL)[,-1]

U10M = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__U10M, Day)
U10M_wide = pivot_wider(U10M, names_from = c(LAT, LON), values_from = DAO_FLDS__U10M)[,-1]

V10M = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__V10M, Day)
V10M_wide = pivot_wider(V10M, names_from = c(LAT, LON), values_from = DAO_FLDS__V10M)[,-1]

UWND = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__UWND, Day)
UWND_wide = pivot_wider(UWND, names_from = c(LAT, LON), values_from = DAO_FLDS__UWND)[,-1]

VWND = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__VWND, Day)
VWND_wide = pivot_wider(VWND, names_from = c(LAT, LON), values_from = DAO_FLDS__VWND)[,-1]

TMPU = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__TMPU, Day)
TMPU_wide = pivot_wider(TMPU, names_from = c(LAT, LON), values_from = DAO_FLDS__TMPU)[,-1]

SPHU = interpretable_data %>% 
  select(LAT, LON, DAO_FLDS__SPHU, Day)
SPHU_wide = pivot_wider(SPHU, names_from = c(LAT, LON), values_from = DAO_FLDS__SPHU)[,-1]

all_preds_all_locs = cbind(CTM_conc_wide, RADSWG_wide, PBL_wide, U10M_wide, V10M_wide, UWND_wide, VWND_wide, TMPU_wide, SPHU_wide)

pred_cov_matrix = cov(all_preds_all_locs)
pred_eigen_decomp = eigen(cov_matrix)
pred_loading_matrix = pred_eigen_decomp$vectors
pred_scores = pred_loading_matrix%*%all_preds_all_locs
cumsum(pred_eigen_decomp$values)/sum(pred_eigen_decomp$values)

label_cov_matrix = cov(Prior_emissions_wide)
label_eigen_decomp = eigen(label_cov_matrix)
label_loading_matrix = label_eigen_decomp$vectors
label_scores = label_loading_matrix%*%Prior_emissions_wide
cumsum(label_eigen_decomp$values)/sum(label_eigen_decomp$values)


```


```{r data split}

indices_to_train = sample(1:nrow(label_scores), round(.8*nrow(label_scores)), replace = FALSE)
X_train = pred_scores[indices_to_sample,]
Y_train = label_scores[indices_to_sample,]
X_val = pred_scores[-indices_to_sample,]
Y_val = label_scores[-indices_to_sample,]

```


```{r model fitting and prediction}

# STILL NEED TO DO 

# training
BRF_models = list()
for (j in 1:ncol(label_scores)){
  
  # might need to change some parameters
  BRF_models[[j]] = train(Y_train[,j]~X_train, 
                         method = "gbm", 
                         trControl = trainControl("cv", 5), 
                         tuneGrid = expand.grid(n.trees = 500,
                                                interaction.depth = 9,
                                                shrinkage = 0.05,
                                                n.minobsinnode = 4))
  
}

# predicting
ML_Estimates = matrix(0, nrow = nrow(Y_test), ncol = ncol(Y_test))
for (j in 1:ncol(label_scores)){
  
  # might need to change some parameters
  ML_Estimates[,j] = predict(BRF_models[[j]], newdata = X_test)
  
}


save(BRF_models, file = "PCA_models.RData") # save the results
save(ML_Estimates, file = "PCA_estimates.RData") # save the results


# # If that's too much memory, we can do something like this: 
# ML_Estimates = matrix(0, nrow = nrow(Y_test), ncol = ncol(Y_test))
# for (j in 1:ncol(label_scores)){
#   
#   # might need to change some parameters
#   jth_BRF_model = train(Y_train[,j]~X_train, 
#                          method = "gbm", 
#                          trControl = trainControl("cv", 5), 
#                          tuneGrid = expand.grid(n.trees = 500,
#                                                 interaction.depth = 9,
#                                                 shrinkage = 0.05,
#                                                 n.minobsinnode = 4))
#   
#     ML_Estimates[,j] = predict(jth_BRF_model, newdata = X_test)
# 
#   
# }
# 
# save(ML_Estimates, file = "PCA_estimates.RData") # save the results



```



```{r data cleaning for plots}

# we hypothesize that the predictions should be close to the posterior NOx emissions
# why? Since we are using the real-life satellite NOx concentrations in those predictions
# The problem: We need to evaluate how close the DAILY predictions are to the MONTHLY posterior estimates

# import functions for cleaning the data to be monthly (to match posteriors). Also includes plotting data
source("plot_scripts.R")

# this function summarizes the daily data by averaging over Year/Month and location. So the ML emission estimates will be averaged over each month, then merged with the posterior
visualization_df = monthly_average_and_combine(results_dataset_using_Satellite, paths_to_posterior_data = c("./cleaned_posterior_data/posterior_estimate_2010.csv", 
  "./cleaned_posterior_data/posterior_estimate_2011.csv"), years = c(2010,2011))

# same thing, but do this with the CTM-based ML estimates. 
visualization_df_using_CTM = monthly_average_and_combine(results_dataset_using_CTM, paths_to_posterior_data = c("./cleaned_posterior_data/posterior_estimate_2010.csv", 
  "./cleaned_posterior_data/posterior_estimate_2011.csv"), years = c(2010,2011))


# merge these two datasets so we don't get confused and overwhelmed...
# now just save the CTM-based predictions to another variable, which explicitly mentions using the CTM concentrations
visualization_df$imputed_ML_Estimates_CTM = visualization_df_using_CTM$imputed_ML_Estimates
visualization_df$added_ML_Estimates_CTM = visualization_df_using_CTM$added_ML_Estimates
# visualization_df$imputed_residuals_CTM = visualization_df_using_CTM$imputed_residuals
# visualization_df$Post_ML_diff_CTM = visualization_df_using_CTM$Post_ML_diff
# visualization_df$Prior_ML_diff_CTM = visualization_df_using_CTM$Prior_ML_diff

# now we only need to use visualization_df for the plots

# save all the work we just did (optional, since honestly the bottleneck is the training)
save(standardized_data, interpretable_data, data_to_train, data_to_predict, data_to_analyze_after_prediction, BRF_model, results_dataset_using_Satellite, results_dataset_using_CTM, visualization_df, file = "east_asia_extra_emissions.RData")

```



```{r EDA and identifying troublepoints}


var_cor_plot = function(x,y,color_vector = NULL, alpha_val = 0.2){
  
  if (is.null(color_vector)){
    plot(x, y, col = alpha("black", alpha = alpha_val),
         xlab = as.character(substitute(x)), 
         ylab = as.character(substitute(y)))
    abline(0,1)
  }else{
    plot(x, y, col = alpha(color_vector, alpha = alpha_val),
         xlab = as.character(substitute(x)), 
         ylab = as.character(substitute(y)))
    abline(0,1)
  }
}



est_cor_plot = function(x,y, adjust = 1, line_size = 1, subset = length(x), log = FALSE){
  
  R = round(cor(x, y), 2)
  NMB = round(sum(x - y)/sum(y), 2)
  NMSE = signif(mean((x-y)^2)/(mean(x)*mean(y))*100.0, 3)
  
  picks = sample(1:length(x), subset)
  
  x_label = as.character(substitute(x))
  y_label = as.character(substitute(y))
  
  x = x[picks]
  y = y[picks]
  
  if(log){x = log(x); y = log(y)}
  
  
  ggplot(mapping = aes(x = x,y = y)) + 
    geom_pointdensity(adjust = adjust) + 
    geom_abline(slope = 1, intercept = 0, size = line_size) +
    xlab(x_label) + 
    ylab(y_label) + 
    ggtitle(paste0("R=", R, ", NMB=", NMB, ", NMSE=", NMSE)) + 
    scale_color_viridis()
  
}


# now using visualization_df, we can make our plots
ML_estimates = visualization_df$imputed_ML_Estimates
ML_estimates_CTM = visualization_df$imputed_ML_Estimates_CTM
Posterior_estimates = visualization_df$Posterior_Estimates
Prior_estimates = visualization_df$imputed_Prior_Estimates


est_cor_plot(Prior_estimates,ML_estimates)

est_cor_plot(Prior_estimates,ML_estimates_CTM) # we guess this would be the best

est_cor_plot(Posterior_estimates,ML_estimates) # we would like this to be the best

est_cor_plot(Posterior_estimates,ML_estimates_CTM)

est_cor_plot(Posterior_estimates,Prior_estimates) # Posterior vs Prior

```




```{r distributional plots}
# concentrations
boxplot_df = data.frame(CTM = visualization_df$CTM_Concentrations, Satellite = visualization_df$Satellite_Concentrations) %>% 
  pivot_longer(cols = 1:2, names_to = "conc_origin", values_to = "units_of_0_to_1")
boxplot(units_of_0_to_1~conc_origin, data = boxplot_df)

# posterior versus prior versus ML versus ML with CTM
boxplot_df = data.frame(Posterior = visualization_df$Posterior_Estimates,
                        Prior = visualization_df$imputed_Prior_Estimates,
                        ML = visualization_df$imputed_ML_Estimates,
                        ML_CTM = visualization_df$imputed_ML_Estimates_CTM) %>% 
  pivot_longer(cols = 1:4, names_to = "Emissions", values_to = "TgN_per_box")
boxplot(TgN_per_box~Emissions, data = boxplot_df)

```







```{r make heatmap}

# now we'd like to make a heatmap that allows us to look at important potential predictors
visualization_df$posterior_vs_ML = visualization_df$Posterior_Estimates - visualization_df$imputed_ML_Estimates # 
visualization_df$prior_vs_posterior = visualization_df$imputed_Prior_Estimates - visualization_df$Posterior_Estimates # prior vs posterior
visualization_df$satellite_effect = visualization_df$imputed_Prior_Estimates - visualization_df$imputed_ML_Estimates # what happens when we plug in satellite data (because Prior = ML with CTM)
visualization_df$fitting_residuals = visualization_df$imputed_Prior_Estimates - visualization_df$imputed_ML_Estimates_CTM # if the model is good


# first, choose variables we are interested in recording
# average them across LAT and LON
heatmap_df = visualization_df %>% 
  group_by(LON, LAT) %>% 
  summarize(var1 = mean(posterior_vs_ML),
            var2 = mean(prior_vs_posterior),
            var3 = mean(satellite_effect),
            var4 = mean(fitting_residuals))
            # count = mean(count))


# read in the .nc file: heatmap_palette.nc (which was originally gctm.Jan1st.2010)
gctm_nc_obj = nc_open("heatmap_palette.nc", write =TRUE)

# read in longitudes, latitudes, and original data for Jan 1st
LON = rep(ncvar_get(gctm_nc_obj, "LON"), gctm_nc_obj$var[["LAT"]]$varsize)
LAT = rep(ncvar_get(gctm_nc_obj, "LAT"), each = gctm_nc_obj$var[["LON"]]$varsize)

# # these are completely optional, and probably should be taken out
# NOx_tracer = ncvar_get(gctm_nc_obj, "IJ-AVG-S__NOx", count = c(-1,-1,4)) # this just has 4 layers. We can only work with these four layers at a time
# orig_CTM_conc = NOx_tracer[,,1]
# orig_CTM_conc_vec = c(orig_CTM_conc)
# orig_sat_conc = NOx_tracer[,,2]
# orig_sat_conc_vec = c(orig_sat_conc)

# combine with our summarized results with their original latitude/longitude layout
# complete_grid = as.data.frame(cbind(LON, LAT, orig_CTM_conc_vec, orig_sat_conc_vec))
complete_grid = as.data.frame(cbind(LON, LAT))#, orig_CTM_conc_vec, orig_sat_conc_vec))
combined_df = left_join(complete_grid, heatmap_df, by = c("LON", "LAT"))

# impute missing data by mean
combined_df$var1 = ifelse(is.na(combined_df$var1), mean(combined_df$var1[!is.na(combined_df$var1)]), combined_df$var1)
combined_df$var2 = ifelse(is.na(combined_df$var2), mean(combined_df$var2[!is.na(combined_df$var2)]), combined_df$var2)
combined_df$var3 = ifelse(is.na(combined_df$var3), mean(combined_df$var3[!is.na(combined_df$var3)]), combined_df$var3)
combined_df$var4 = ifelse(is.na(combined_df$var4), mean(combined_df$var4[!is.na(combined_df$var4)]), combined_df$var4)

# choose the 4 layers we want to view on panopto
mydata_array1 = combined_df$var1
mydata_array2 = combined_df$var2
mydata_array3 = combined_df$var3
mydata_array4 = combined_df$var4
# mydata_array3 = combined_df$avg_pop_density
# mydata_array4 = combined_df$avg_pop_count

# put them together, then put them in the nc variable by overwriting the original information
total_data = c(mydata_array1, mydata_array2, mydata_array3, mydata_array4)
ncvar_put(gctm_nc_obj, gctm_nc_obj$var$`IJ-AVG-S__NOx`$name, total_data)
nc_close(gctm_nc_obj)

```














